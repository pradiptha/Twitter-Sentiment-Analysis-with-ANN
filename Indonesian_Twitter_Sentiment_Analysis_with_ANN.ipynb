{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indonesian Twitter Sentiment Analysis with ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Sxof29zcP0GPraxEWwk_uzRbmyIv7Ss3",
      "authorship_tag": "ABX9TyNPaax+vsGO5Sy0Tr+o0jRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradiptha/files/blob/master/Indonesian_Twitter_Sentiment_Analysis_with_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb_kwT5UFIAJ",
        "colab_type": "text"
      },
      "source": [
        "**Analisis Sentimen terhadap data tweet berbahasa Indonesia dengan label (-1, 0, 1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CqrnVNzGvt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('downloading data...')\n",
        "# !wget https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv\n",
        "# !wget https://raw.githubusercontent.com/pradiptha/files/master/Indonesian%20Sentiment%20Twitter%20Dataset%20Labeled.csv\n",
        "# !wget https://raw.githubusercontent.com/pradiptha/files/master/Preproccessed-data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C5e7rrfPm6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sastrawi\n",
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAegWo1lW4WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from multiprocessing import Pool\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import path\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "listStopword = set(stopwords.words('indonesian'))\n",
        "factory = StemmerFactory()\n",
        "stemmerIndonesia = factory.create_stemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hub4VCOIOWRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readDataset(filename):\n",
        "    df = pd.read_csv(filename, sep=\"\\t\")\n",
        "    target = list(df[\"sentimen\"])\n",
        "    data = list(df[\"Tweet\"])\n",
        "    target = target\n",
        "    data = data\n",
        "    return data, target\n",
        "\n",
        "def preProcessing(tweet):\n",
        "    tweet = tweet.lower()  # Convert text to lowercase\n",
        "    tweet = re.sub(r'\\d+', '', tweet)  # Numbers removing\n",
        "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)  # funnnnny --> funny\n",
        "    tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet)\n",
        "    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tweet = tweet.strip(r'\\n')\n",
        "    tweet = tweet.strip(' ')  # White spaces removal\n",
        "    tweet = normalization(tweet)\n",
        "    # tweet = nltk.tokenize.word_tokenize(tweet)\n",
        "    filtering = [i for i in list(tweet) if not i in listStopword]\n",
        "    tweet = [term for term in filtering if (term != \"\" and term != \"-\")]\n",
        "    tweet = \" \".join(tweet)\n",
        "    tweet = stemmerIndonesia.stem(tweet)\n",
        "    return tweet\n",
        "\n",
        "def savePreProcessing(sentimen, tweet):\n",
        "    sentimen_a = []\n",
        "    tweet_a = []\n",
        "    for i in range(len(tweet)):\n",
        "        if tweet[i] is not \"\":\n",
        "            tweet_a.append(tweet[i])\n",
        "            sentimen_a.append(sentimen[i])\n",
        "\n",
        "    data = {'sentimen': sentimen_a, 'Tweet': tweet_a}\n",
        "    print(\"saving preprocessed data ...\")\n",
        "    pd.DataFrame(data).to_csv(\"Preproccessed-data.csv\", sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "def normalization(tweet):\n",
        "    df = pd.read_csv(\"colloquial-indonesian-lexicon.csv\", sep=\",\")\n",
        "    slang = list(df[\"slang\"])\n",
        "    formal = list(df[\"formal\"])\n",
        "    tweet = tweet.split()\n",
        "    new_tweet = []\n",
        "    for i in tweet:\n",
        "        for index, j in enumerate(slang):\n",
        "            if i == j:\n",
        "                i = formal[index]\n",
        "        new_tweet.append(i)\n",
        "    # new_tweet = ' '.join(new_tweet)\n",
        "    return(new_tweet)\n",
        "\n",
        "def fiturExtract(x_train, x_test):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
        "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
        "    return x_train_tfidf, x_test_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAX-meEVXvi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "7d94413e-c57d-4927-ae09-16e2872cb1f3"
      },
      "source": [
        "print(\"Sentiment Analysis Using Machine Learning\")\n",
        "print(\"-----------------------------------------\\n\")\n",
        "print(\"Reading the DataSet...\")\n",
        "if path.exists('Preproccessed-data.csv'):\n",
        "    tweets, target = readDataset('Preproccessed-data.csv')\n",
        "else:\n",
        "    data, target = readDataset('Indonesian Sentiment Twitter Dataset Labeled.csv')\n",
        "    print(\"-----------------------------------------\\n\")\n",
        "    print(\"Preprocessing the DataSet. This may take some time...\")\n",
        "    num_cpus = psutil.cpu_count()\n",
        "    p = Pool(num_cpus)\n",
        "    tweets = p.map(preProcessing, data)\n",
        "    p.close()\n",
        "    savePreProcessing(target, tweets)\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment Analysis Using Machine Learning\n",
            "-----------------------------------------\n",
            "\n",
            "Reading the DataSet...\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZp4XUYxcppN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "897eb3b6-2218-43eb-866d-6b3a779dff05"
      },
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(target)\n",
        "encoded_Y = encoder.transform(target)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = to_categorical(encoded_Y)\n",
        "print('sebelum\\n', target[:5])\n",
        "print('sesudah\\n', dummy_y[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sebelum\n",
            " [-1, -1, 1, 1, -1]\n",
            "sesudah\n",
            " [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifeL4_6OYsQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c35e2bd8-e01a-4711-9270-304033f36b18"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, dummy_y, test_size=0.3, random_state=10)\n",
        "print(f\"Data train {len(x_train)} entries, Data test {len(x_test)} entries\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data train 7491 entries, Data test 3211 entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfXBZk_6YuCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_tfidf, x_test_tfidf = fiturExtract(x_train, x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j32qzkTkYwk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "a98fe1a3-bedf-47f8-fd40-24f74a06c84c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=x_train_tfidf.shape[1]))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 128)               1661952   \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 1,670,403\n",
            "Trainable params: 1,670,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8bW-PeERLwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "bd6d8bb9-8400-457e-c4c5-526d72509da6"
      },
      "source": [
        "model.fit(x_train_tfidf, y_train, batch_size=128, epochs=10, validation_data=(x_test_tfidf, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7491 samples, validate on 3211 samples\n",
            "Epoch 1/10\n",
            "7491/7491 [==============================] - 3s 368us/step - loss: 1.0345 - accuracy: 0.4905 - val_loss: 0.9776 - val_accuracy: 0.4858\n",
            "Epoch 2/10\n",
            "7491/7491 [==============================] - 3s 346us/step - loss: 0.7632 - accuracy: 0.6528 - val_loss: 0.8885 - val_accuracy: 0.6145\n",
            "Epoch 3/10\n",
            "7491/7491 [==============================] - 3s 355us/step - loss: 0.3730 - accuracy: 0.8690 - val_loss: 1.0728 - val_accuracy: 0.6104\n",
            "Epoch 4/10\n",
            "7491/7491 [==============================] - 3s 347us/step - loss: 0.2015 - accuracy: 0.9325 - val_loss: 1.2659 - val_accuracy: 0.5939\n",
            "Epoch 5/10\n",
            "7491/7491 [==============================] - 3s 353us/step - loss: 0.1362 - accuracy: 0.9545 - val_loss: 1.4111 - val_accuracy: 0.5830\n",
            "Epoch 6/10\n",
            "7491/7491 [==============================] - 3s 353us/step - loss: 0.1045 - accuracy: 0.9625 - val_loss: 1.5385 - val_accuracy: 0.5780\n",
            "Epoch 7/10\n",
            "7491/7491 [==============================] - 3s 350us/step - loss: 0.0877 - accuracy: 0.9670 - val_loss: 1.6710 - val_accuracy: 0.5755\n",
            "Epoch 8/10\n",
            "7491/7491 [==============================] - 3s 346us/step - loss: 0.0772 - accuracy: 0.9709 - val_loss: 1.7596 - val_accuracy: 0.5780\n",
            "Epoch 9/10\n",
            "7491/7491 [==============================] - 3s 348us/step - loss: 0.0684 - accuracy: 0.9732 - val_loss: 1.8105 - val_accuracy: 0.5712\n",
            "Epoch 10/10\n",
            "7491/7491 [==============================] - 3s 351us/step - loss: 0.0638 - accuracy: 0.9744 - val_loss: 1.8744 - val_accuracy: 0.5702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5c2a2a7b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAYw1ocnhUQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict_classes(x_test_tfidf)\n",
        "encoder.fit(y_pred)\n",
        "encoded_Y = encoder.transform(y_pred)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_pred = to_categorical(encoded_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGhAmGtIpnZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "f84bd461-fb8d-4d5a-db18-dd242932619e"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test_tfidf, y_test)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "3211/3211 [==============================] - 0s 129us/step\n",
            "test loss, test acc: [1.8743639431872943, 0.570227324962616]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35YiJUnTt1pM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "19f136e3-7bc9-44c3-e75a-16138486bf1c"
      },
      "source": [
        "y_test_s = np.argmax(y_test, axis=1)\n",
        "y_pred_s = np.argmax(y_pred, axis=1)\n",
        "y_test_s = [i - 1 for i in y_test_s]\n",
        "y_pred_s = [i - 1 for i in y_pred_s]\n",
        "print(\"Accuracy_score : \", accuracy_score(y_test_s, y_pred_s))\n",
        "print(\"Confusion_matrix : \\n\", confusion_matrix(y_test_s, y_pred_s))\n",
        "print(classification_report(y_test_s, y_pred_s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy_score :  0.5702273435066957\n",
            "Confusion_matrix : \n",
            " [[500 283 120]\n",
            " [332 994 233]\n",
            " [156 256 337]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.51      0.55      0.53       903\n",
            "           0       0.65      0.64      0.64      1559\n",
            "           1       0.49      0.45      0.47       749\n",
            "\n",
            "    accuracy                           0.57      3211\n",
            "   macro avg       0.55      0.55      0.55      3211\n",
            "weighted avg       0.57      0.57      0.57      3211\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGPv1bRanNiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "87c6c99f-d403-4517-928d-0b0b020053ea"
      },
      "source": [
        "x_train_tfidf.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVDqD4zSxF2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "3ddefb97-8ada-4330-fc3c-41f355b8bf9d"
      },
      "source": [
        "df = pd.read_csv('Indonesian Sentiment Twitter Dataset Labeled.csv', sep='\\t')\n",
        "sentimen = list(df['sentimen'])\n",
        "tweet = list(df['Tweet'])\n",
        "for i in range(len(sentimen)):\n",
        "  if sentimen[i] == -1:\n",
        "    print(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}